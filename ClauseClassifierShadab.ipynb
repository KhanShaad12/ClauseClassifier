{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQyPvY+klACiaEnz6+3i9f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ON9_vLfFNObE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea996e80-69ed-424a-e1e3-42c3610f881d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Text Label\n",
            "0   1. Limitation of Liability: Do not agree to a ...   Bad\n",
            "1   2. Pay-when/if-paid requirement in a flow down...   Bad\n",
            "2   a. Where contract client is in long-standing g...   Bad\n",
            "3   b. For other architects/contractors/primes whe...   Bad\n",
            "4   c. For other architects/contractors/primes whe...   Bad\n",
            "5   a. If the client doesn’t agree, we escalate to...   Bad\n",
            "6           3. Limitations on Working for Competitors   Bad\n",
            "7   a. Always push back, propose accommodating lan...   Bad\n",
            "8   4. Payment Terms turnaround time that is beyon...   Bad\n",
            "9   a. Up to 45 days payment terms are acceptable ...   Bad\n",
            "10  b. Greater than 45 days must get chief financi...   Bad\n",
            "11  · Chief financial office to consult chief reve...   Bad\n",
            "12                               5. Background Checks   Bad\n",
            "13  a. Use the client’s provider. Review the clien...   Bad\n",
            "14  · If necessary, escalate to chief financial of...   Bad\n",
            "15  6. Non-solicitation - Refusal to agree to any ...   Bad\n",
            "16  a. language when the client doesn’t accept our...   Bad\n",
            "17  The parties shall not solicit for employment o...   Bad\n",
            "18  · Fixed fee ≠ %. If we can’t agree to a mutual...   Bad\n",
            "19  7. Indemnity/Defense Obligations that could cr...   Bad\n",
            "20  a. Conditioning our Indemnification Obligation...   Bad\n",
            "21  8. Elevated standard of care and “warranty” la...   Bad\n",
            "22  a. Strike warranty obligations and revise as “...   Bad\n",
            "23  9. IT/Data Security standards we don’t meet (s...   Bad\n",
            "24  a. Continue to send to IT director or Chief In...   Bad\n",
            "25  10. Onerous Prime Agreement (obligations other...   Bad\n",
            "26  a. We’ll escalate to chief financial officer, ...   Bad\n",
            "27  11. W/M/DBE goals – Obligation to use good fai...   Bad\n",
            "28  12. Attorney’s Fees – The prevailing party get...   Bad\n",
            "29  13. Payment Portals – Required subscription + ...   Bad\n",
            "30  a. chief financial officer approves subscripti...   Bad\n",
            "31  will not pay for (individual) invoice upload. ...   Bad\n",
            "32  · In the event a biz decision is made to agree...   Bad\n",
            "33  · The sales Team must be more mindful of this ...   Bad\n",
            "34  · This must become a standard inquiry during t...   Bad\n",
            "35  14. Audit Fees – Paying for audit fees if an a...   Bad\n",
            "36  a. We agree to pay any owed funds discovered d...   Bad\n",
            "37  15. Insurance – Refer to COI, coverage types a...   Bad\n",
            "38                                          16. Other   Bad\n",
            "39  a. Striking “Time is of the essence” obligations.   Bad\n",
            "40  b. Striking terms which reference work to be p...   Bad\n",
            "41  c. Inserting language that limits/removes any ...   Bad\n",
            "42  d. Termination clause – usually as a right aft...   Bad\n",
            "43  e. Contract Templates geared toward contractor...   Bad\n",
            "44  Means & Methods. It is understood and agreed t...  Good\n",
            "45  Termination. The Consultant may terminate this...  Good\n",
            "46  Limitation of Liability. To the fullest extent...  Good\n",
            "47  ADA / FHA Compliance. ÚDA/FHA Compliance. It i...  Good\n",
            "48  Background Checks. Consultant performs employe...  Good\n",
            "49  Indemnity. Subconsultant shall not be obligate...  Good\n",
            "50  This indemnity does not include an upfront dut...  Good\n"
          ]
        }
      ],
      "source": [
        "#LOAD TRAINING DATA\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "df = pd.read_csv('./trainclauses.csv')\n",
        "df.head()\n",
        "print(df)\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GENERATE ENCODINGS\n",
        "def process_data(row):\n",
        "\n",
        "    text = row['Text']\n",
        "    text = str(text)\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    encodings = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    label = 0\n",
        "    if row['Label'] == 'Good':\n",
        "        label += 1\n",
        "\n",
        "    encodings['label'] = label\n",
        "    encodings['text'] = text\n",
        "\n",
        "    return encodings\n",
        "processed_data = []\n",
        "#print(df)\n",
        "for i in range(len(df[:1000])):\n",
        "    processed_data.append(process_data(df.iloc[i]))\n",
        "print(len(processed_data))\n",
        "\n",
        "# SPLIT THE DATAFRMES\n",
        "new_df = pd.DataFrame(processed_data)\n",
        "\n",
        "train_df, valid_df = train_test_split(\n",
        "    new_df,\n",
        "    test_size=0.2,\n",
        "    random_state=2022\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxvMndMZNR6W",
        "outputId": "da8d1fc2-f9be-4a8d-b733-1b46032e903e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GENERATE DATASETS\n",
        "import pyarrow as pa\n",
        "from datasets import Dataset\n",
        "\n",
        "train_hg = Dataset(pa.Table.from_pandas(train_df))\n",
        "valid_hg = Dataset(pa.Table.from_pandas(valid_df))"
      ],
      "metadata": {
        "id": "dpJtsM_pRjPj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORT PRETRAINED MODEL\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWMf_kg-RlPi",
        "outputId": "5f25c12f-1d4f-4e07-8f0c-a95faea9f257"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#INITIALIZE MODEL TO TRAIN\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"./result\", evaluation_strategy=\"epoch\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_hg,\n",
        "    eval_dataset=valid_hg,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "8VswI6GhSUuD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN THE MODEL\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "8oDFXI3PTp9S",
        "outputId": "23872092-664e-4519-a1b1-3a907a875819"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 02:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.492252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.470348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.337617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15, training_loss=0.3443155288696289, metrics={'train_runtime': 183.734, 'train_samples_per_second': 0.653, 'train_steps_per_second': 0.082, 'total_flos': 7893331660800.0, 'train_loss': 0.3443155288696289, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EVALUATE MODEL\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "l7xGuKG3Udjy",
        "outputId": "9df4804f-6aaf-4ff2-c61f-515538fe21a4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.33761662244796753,\n",
              " 'eval_runtime': 4.0584,\n",
              " 'eval_samples_per_second': 2.71,\n",
              " 'eval_steps_per_second': 0.493,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SAVE THE TRAINED MODEL\n",
        "model.save_pretrained('./model/')"
      ],
      "metadata": {
        "id": "nnAC-hlqUnxo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD THE SAVE CUSTOM TRAINED MODEL\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "new_model = AutoModelForSequenceClassification.from_pretrained('./model/').to(device)"
      ],
      "metadata": {
        "id": "dMPtZZpBUsvQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "new_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "dU5EDZofVJC3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOGIC TO GET PREDICTION\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_prediction(text):\n",
        "    encoding = new_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "    outputs = new_model(**encoding)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    print(sigmoid)\n",
        "    probs = sigmoid(logits.squeeze().cpu())\n",
        "    probs = probs.detach().numpy()\n",
        "    label = np.argmax(probs, axis=-1)\n",
        "\n",
        "    if label == 1:\n",
        "        return {\n",
        "            'tag': 'Good',\n",
        "            'probability': probs[1]\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'tag': 'Bad',\n",
        "            'probability': probs[0]\n",
        "        }"
      ],
      "metadata": {
        "id": "muNxW2lCVPbJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(\"The parties are independent contractors, and nothing contained in this Agreement will be construed as creating any agency, partnership, joint venture or other form of joint enterprise, employment, or fiduciary relationship between them.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXr3m349VdSR",
        "outputId": "1ac4a817-af5d-43ea-a402-0c6c632378fd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tag': 'Good', 'probability': 0.5504104}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}